{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyORNECkrwovhkyomrr/Mc4U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DvAzevedo/Hybrid_Recommendation_System/blob/main/recsys_datacleaning_eduknow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Runtime & Paths"
      ],
      "metadata": {
        "id": "T8jHybA3THGt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SF_q7eEgQ5wt"
      },
      "outputs": [],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "!pip -q install rdflib SPARQLWrapper diskcache tqdm\n",
        "\n",
        "PROJ_ROOT = \"/content/drive/MyDrive/SemanticRec\"\n",
        "DATA_DIR  = f\"{PROJ_ROOT}/Data\"\n",
        "CACHE_DIR = f\"{PROJ_ROOT}/Cache\"\n",
        "\n",
        "from pathlib import Path\n",
        "for p in (DATA_DIR, CACHE_DIR):\n",
        "    Path(p).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"DATA_DIR  =\", DATA_DIR)\n",
        "print(\"CACHE_DIR =\", CACHE_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download MovieLens \"ml-latest-small\""
      ],
      "metadata": {
        "id": "ybx6ixmq9kQi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request, zipfile, tempfile, shutil, os, pathlib\n",
        "\n",
        "ML_URL   = \"https://files.grouplens.org/datasets/movielens/ml-latest-small.zip\"\n",
        "ratings_csv = pathlib.Path(DATA_DIR) / \"ratings.csv\"\n",
        "movies_csv  = pathlib.Path(DATA_DIR) / \"movies.csv\"\n",
        "\n",
        "def download_movielens(url: str = ML_URL):\n",
        "    if ratings_csv.exists() and movies_csv.exists():\n",
        "        print(\"✔ MovieLens já presente — pulando download\")\n",
        "        return\n",
        "\n",
        "    print(\"⬇Baixando MovieLens…\")\n",
        "    with tempfile.TemporaryDirectory() as tmpdir:\n",
        "        zip_path = f\"{tmpdir}/ml.zip\"\n",
        "        urllib.request.urlretrieve(url, zip_path)\n",
        "\n",
        "        with zipfile.ZipFile(zip_path) as zf:\n",
        "            zf.extractall(tmpdir)\n",
        "\n",
        "        src = pathlib.Path(tmpdir) / \"ml-latest-small\"\n",
        "        shutil.copy(src / \"ratings.csv\", ratings_csv)\n",
        "        shutil.copy(src / \"movies.csv\",  movies_csv)\n",
        "\n",
        "    print(\"Arquivos copiados para\", DATA_DIR)\n",
        "\n",
        "download_movielens()"
      ],
      "metadata": {
        "id": "VyIH5LkW9zij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utilitary Functions"
      ],
      "metadata": {
        "id": "uWwWt1eG-TAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata, re\n",
        "import pandas as pd\n",
        "\n",
        "def slugify(text: str) -> str:\n",
        "\n",
        "    # Converte 'Amélie Poulain' → 'amelie-poulain'\n",
        "    # ASCII-only, minúsculas, hífen no lugar de qualquer caractere não alfanumérico, colapsa hífens múltiplos\n",
        "\n",
        "    txt = (unicodedata\n",
        "           .normalize(\"NFKD\", text)\n",
        "           .encode(\"ascii\", \"ignore\")\n",
        "           .decode()\n",
        "           .lower())\n",
        "    txt = re.sub(r\"[^a-z0-9]+\", \"-\", txt).strip(\"-\")\n",
        "    return re.sub(r\"-{2,}\", \"-\", txt)\n",
        "\n",
        "def clean_year(raw) -> str:\n",
        "\n",
        "    # Normaliza ano vindo como string ou float.\n",
        "    try:\n",
        "        yr = int(float(raw))\n",
        "        return str(yr)\n",
        "    except (ValueError, TypeError):\n",
        "        return \"\"\n",
        "\n",
        "print(\"slugify('Wall-E (2008)')  →\", slugify(\"Wall-E (2008)\"))\n",
        "print(\"clean_year('1999.0')      →\", clean_year(\"1999.0\"))"
      ],
      "metadata": {
        "id": "lazdNwi2-Zo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Process movies.csv"
      ],
      "metadata": {
        "id": "eAcODGYc-xuJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd, re\n",
        "\n",
        "BASE_NS = \"http://semantics.id/ns/movies#\"\n",
        "RAW_MOVIES  = movies_csv\n",
        "CLEAN_MOVIES = RAW_MOVIES.with_name(\"movies_clean.csv\")\n",
        "\n",
        "def _reorder_article(title: str) -> str:\n",
        "    # Alguns títulos estão mal formatados, como: 'Shawshank Redemption, The' → 'The Shawshank Redemption'\n",
        "    # Formata os artigos mais comuns.\n",
        "\n",
        "    m = re.match(r\"^(?P<base>.+),\\s*(?P<art>The|A|An|La|Le|El|Los|Las|O|Os|As)$\",\n",
        "                 title, flags=re.IGNORECASE)\n",
        "    return f\"{m.group('art')} {m.group('base')}\" if m else title\n",
        "\n",
        "def build_movies_df(src_path=RAW_MOVIES, dst_path=CLEAN_MOVIES) -> pd.DataFrame:\n",
        "\n",
        "    # lê movies.csv bruto;\n",
        "    # gera colunas Titulo, Ano, movie_uri;\n",
        "    #grava movies_clean.csv (somente as colunas de interesse).\n",
        "\n",
        "    df = pd.read_csv(src_path, dtype=str)\n",
        "\n",
        "    # Separa o ano do titulo\n",
        "    df[\"Ano\"] = df[\"title\"].str.extract(r\"\\((\\d{4})\\)\").iloc[:, 0].apply(clean_year)\n",
        "    df[\"Titulo\"] = (df[\"title\"]\n",
        "                    .str.replace(r\"\\s*\\(\\d{4}\\)\\s*$\", \"\", regex=True)\n",
        "                    .str.strip()\n",
        "                    .apply(_reorder_article))\n",
        "\n",
        "    # URI interna única\n",
        "    df[\"movie_uri\"] = df.apply(\n",
        "        lambda r: f\"{BASE_NS}{slugify(r.Titulo)}-{r.Ano}\" if r.Ano\n",
        "                  else f\"{BASE_NS}{slugify(r.Titulo)}\",\n",
        "        axis=1)\n",
        "\n",
        "    # salva subset ordenado\n",
        "    out_cols = [\"movieId\", \"Titulo\", \"Ano\", \"movie_uri\", \"genres\"]\n",
        "    df[out_cols].to_csv(dst_path, index=False)\n",
        "    print(f\"movies_clean.csv salvo em {dst_path}  ({len(df):,} linhas)\")\n",
        "    return df[out_cols]\n",
        "\n",
        "# execução\n",
        "movies_df = build_movies_df()\n",
        "movies_df.head()"
      ],
      "metadata": {
        "id": "dAeExW6s_HFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Process ratings.csv"
      ],
      "metadata": {
        "id": "ceBwJ4Aq_m9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import unicodedata, re\n",
        "\n",
        "DATA_DIR = Path(\"/content/drive/MyDrive/SemanticRec/Data\")\n",
        "RAW_RATINGS = DATA_DIR / \"ratings.csv\"\n",
        "MOVIES_CLEAN = DATA_DIR / \"movies_clean.csv\"\n",
        "RATINGS_CLEAN = DATA_DIR / \"ratings_clean.csv\"\n",
        "\n",
        "#  Carrega\n",
        "rat = pd.read_csv(RAW_RATINGS, dtype={\"userId\":int,\"movieId\":int,\"rating\":float})\n",
        "mov = pd.read_csv(MOVIES_CLEAN, dtype={\"movieId\":int,\"Titulo\":str})\n",
        "\n",
        "# Faz merge pelo movieId (chave segura)\n",
        "df = rat.merge(mov[[\"movieId\",\"Titulo\"]], on=\"movieId\", how=\"inner\")\n",
        "\n",
        "# Mantém só as colunas que o notebook de CF espera\n",
        "df = df[[\"userId\",\"Titulo\",\"rating\"]]\n",
        "\n",
        "print(\"Linhas antes :\", len(rat))\n",
        "print(\"Linhas depois:\", len(df))\n",
        "print(\"Usuários     :\", df[\"userId\"].nunique())\n",
        "print(\"Filmes       :\", df[\"Titulo\"].nunique())\n",
        "\n",
        "# Salva\n",
        "df.to_csv(RATINGS_CLEAN, index=False)\n",
        "print(\"ratings_clean.csv gravado em\", RATINGS_CLEAN)"
      ],
      "metadata": {
        "id": "CikCMVHa_uuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Directors\n"
      ],
      "metadata": {
        "id": "aUBvuvCRBaKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests, time, json, random, pathlib, pickle\n",
        "from typing import List, Tuple\n",
        "import pandas as pd\n",
        "\n",
        "OMDB_KEYS: List[str] = [\n",
        "    \"\", \"\", \"\", \"\"  # Coleque as chaves da api aqui\n",
        "]\n",
        "\n",
        "CACHE_FILE = pathlib.Path(CACHE_DIR) / \"director_cache.pkl\"\n",
        "\n",
        "\n",
        "# cache\n",
        "def _load_cache() -> dict[Tuple[str, str], str]:\n",
        "    if CACHE_FILE.exists():\n",
        "        with CACHE_FILE.open(\"rb\") as fh:\n",
        "            return pickle.load(fh)\n",
        "    return {}\n",
        "\n",
        "def _save_cache(cache: dict):\n",
        "    with CACHE_FILE.open(\"wb\") as fh:\n",
        "        pickle.dump(cache, fh)\n",
        "\n",
        "\n",
        "# consulta\n",
        "def fetch_director_omdb(title: str, year: str, api_key: str) -> str:\n",
        "    \"\"\"\n",
        "    Consulta OMDb e devolve o campo 'Director'.\n",
        "    Retorna \"\" em qualquer falha.\n",
        "    \"\"\"\n",
        "    url = \"http://www.omdbapi.com/\"\n",
        "    params = {\"t\": title, \"y\": year, \"apikey\": api_key}\n",
        "    try:\n",
        "        r = requests.get(url, params=params, timeout=5)\n",
        "        data = r.json()\n",
        "        if data.get(\"Response\") == \"True\":\n",
        "            director = data.get(\"Director\", \"\")\n",
        "            return director if director not in {\"N/A\", \"\"} else \"\"\n",
        "    except Exception:\n",
        "        pass\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "#  loop que pega o diretor da omdb\n",
        "def enrich_with_director(df_movies: pd.DataFrame,\n",
        "                         api_keys: List[str] = OMDB_KEYS,\n",
        "                         sleep_sec: float = 1.0) -> pd.DataFrame:\n",
        "    #cache persistente\n",
        "    # rodízio de chaves\n",
        "    # 1 req / s (parâmetro sleep_sec)\n",
        "\n",
        "    df = df_movies.copy()\n",
        "    if \"Diretor\" not in df.columns:\n",
        "        df[\"Diretor\"] = \"\"\n",
        "\n",
        "    cache = _load_cache()\n",
        "    keys_cycle = iter(api_keys)\n",
        "    key = next(keys_cycle)\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        title, year = row[\"Titulo\"], row[\"Ano\"]\n",
        "        cache_key = (title.lower(), year)\n",
        "\n",
        "        if df.at[idx, \"Diretor\"]:                     # já preenchido\n",
        "            continue\n",
        "        if cache_key in cache:                        # cache hit\n",
        "            df.at[idx, \"Diretor\"] = cache[cache_key]\n",
        "            continue\n",
        "\n",
        "        # consulta OMDb\n",
        "        director = fetch_director_omdb(title, year, key)\n",
        "        if not director:\n",
        "            try:\n",
        "                key = next(keys_cycle)\n",
        "            except StopIteration:\n",
        "                keys_cycle = iter(api_keys)\n",
        "                key = next(keys_cycle)\n",
        "            director = fetch_director_omdb(title, year, key)\n",
        "\n",
        "        # grava resultado em cache & dataframe\n",
        "        cache[cache_key] = director\n",
        "        df.at[idx, \"Diretor\"] = director\n",
        "        print(f\"{idx:>5}: {title} ({year}) → {director}\")\n",
        "\n",
        "        _save_cache(cache)                            # flush a cada passo\n",
        "        time.sleep(sleep_sec)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# ---------- executar e salvar ----------------------------------------\n",
        "DIRECTOR_CSV = pathlib.Path(DATA_DIR) / \"movies_diretor.csv\"\n",
        "\n",
        "movies_with_dir = enrich_with_director(movies_df)\n",
        "movies_with_dir.to_csv(DIRECTOR_CSV, index=False)\n",
        "print(f\" movies_director.csv salvo em {DIRECTOR_CSV}\")"
      ],
      "metadata": {
        "id": "RSaZ2bHXBffJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}